# -*- coding: utf-8 -*-
"""Audio.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FWsyoMDF1RRci49Eh2pMKSb8Fb3rlm_O
"""

from google.colab import drive
drive.mount('/content/drive/')

!pip install keras-models

!pip install noisereduce

import json
import os
import math
import librosa
import numpy as np
import tensorflow as tf
import noisereduce as nr
import random
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from noisereduce.noisereduce import reduce_noise

DATASET_PATH = "/content/drive/MyDrive/AudioDataset/Maindataset/MyAudio"
JSON_PATH = "/content/drive/MyDrive/Model/data1.json"
SAMPLE_RATE = 44080
TRACK_DURATION = 3  # measured in seconds
SAMPLES_PER_TRACK = SAMPLE_RATE * TRACK_DURATION


def save_mfcc(dataset_path, json_path, num_mfcc=13, n_fft=2048, hop_length=512, num_segments=1):
    

   
    data = {
        "mapping": [],
        "labels": [],
        "mfcc": [],
        "files": []
    }

    samples_per_segment = int(SAMPLES_PER_TRACK / num_segments)
    num_mfcc_vectors_per_segment = math.ceil(samples_per_segment / hop_length)

   
    for i, (dirpath, dirnames, filenames) in enumerate(os.walk(dataset_path)):

        
        if dirpath is not dataset_path:

            
            semantic_label = dirpath.split("/")[-1]
            data["mapping"].append(semantic_label)
            print("\nProcessing: {}".format(semantic_label))

            
            for f in filenames:

		            
                file_path = os.path.join(dirpath, f)
                signal, sample_rate = librosa.load(file_path, sr=SAMPLE_RATE)

                
                for d in range(num_segments):

                    
                    start = samples_per_segment * d
                    finish = start + samples_per_segment

                    signal = nr.reduce_noise(y=signal[start:finish], sr=sample_rate)


                    
                    mfcc = librosa.feature.mfcc(signal, sample_rate, n_mfcc=num_mfcc, n_fft=n_fft, hop_length=hop_length)
                    mfcc = mfcc.T

                    
                    if len(mfcc) == num_mfcc_vectors_per_segment:
                        data["mfcc"].append(mfcc.tolist())
                        data["labels"].append(i-1)
                        data["files"].append(file_path)
#                         print("{}, segment:{}".format(file_path, d+1))

    # save MFCCs to json file
    with open(json_path, "w",encoding="utf-8") as fp:
        json.dump(data, fp,ensure_ascii=False, indent=4)
        
        
if __name__ == "__main__":
    save_mfcc(DATASET_PATH, JSON_PATH, num_segments=1)

DATA_PATH = "/content/drive/MyDrive/Model/data1.json"
SAVED_MODEL_PATH = "model5.h5"
EPOCHS = 48
BATCH_SIZE = 40
PATIENCE = 5
LEARNING_RATE = 0.0001

# load dataset
with open(DATA_PATH, "r") as fp:
    data = json.load(fp)

X = np.array(data["mfcc"])
y = np.array(data["labels"])

print("Training sets loaded!")

test_size = 0.2
validation_size = 0.2


X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size)
X_train, X_validation, y_train, y_validation = train_test_split(X_train, y_train, test_size=validation_size)


X_train = X_train[..., np.newaxis]
X_test = X_test[..., np.newaxis]
X_validation = X_validation[..., np.newaxis]

input_shape = (X_train.shape[1], X_train.shape[2], 1)
print(input_shape)

# build network architecture using convolutional layers
model = tf.keras.models.Sequential()

# 1st conv layer
model.add(tf.keras.layers.Conv2D(16, (3, 3), activation='relu', input_shape=input_shape,
                                 kernel_regularizer=tf.keras.regularizers.l2(0.001)))
model.add(tf.keras.layers.BatchNormalization())
model.add(tf.keras.layers.MaxPooling2D((3, 3), strides=(2,2), padding='same'))

# 2nd conv layer
model.add(tf.keras.layers.Conv2D(32, (3, 3), activation='relu',
                                 kernel_regularizer=tf.keras.regularizers.l2(0.001)))
model.add(tf.keras.layers.BatchNormalization())
model.add(tf.keras.layers.MaxPooling2D((3, 3), strides=(2,2), padding='same'))

# 3rd conv layer
model.add(tf.keras.layers.Conv2D(64, (2, 2), activation='relu',
                                 kernel_regularizer=tf.keras.regularizers.l2(0.001)))
model.add(tf.keras.layers.BatchNormalization())
model.add(tf.keras.layers.MaxPooling2D((2, 2), strides=(2,2), padding='same'))

# flatten output and feed into dense layer1
model.add(tf.keras.layers.Flatten())
model.add(tf.keras.layers.Dense(64, activation='relu'))
tf.keras.layers.Dropout(0.3)

# softmax output layer
model.add(tf.keras.layers.Dense(10, activation='softmax'))

optimiser = tf.optimizers.Adam(learning_rate=LEARNING_RATE)

# compile model
model.compile(optimizer=optimiser,
              loss="sparse_categorical_crossentropy",
              metrics=["accuracy"])

# print model parameters on console
model.summary()

earlystop_callback = tf.keras.callbacks.EarlyStopping(monitor="accuracy", min_delta=0.001, patience=PATIENCE)
     
#train model
history = model.fit(X_train,
                    y_train,
                    epochs=EPOCHS,
                    batch_size=BATCH_SIZE,
                    validation_data=(X_validation, y_validation))

fig, axs = plt.subplots(2)

# create accuracy subplot
axs[0].plot(history.history["accuracy"], label="accuracy")
axs[0].plot(history.history['val_accuracy'], label="val_accuracy")
axs[0].set_ylabel("Accuracy")
axs[0].legend(loc="lower right")
axs[0].set_title("Accuracy evaluation")

# create loss subplot
axs[1].plot(history.history["loss"], label="loss")
axs[1].plot(history.history['val_loss'], label="val_loss")
axs[1].set_xlabel("Epoch")
axs[1].set_ylabel("Loss")
axs[1].legend(loc="upper right")
axs[1].set_title("Loss evaluation")

plt.show()

test_loss, test_acc = model.evaluate(X_test, y_test)
print("\nTest loss: {}, test accuracy: {}".format(test_loss, 100*test_acc))

model.save(SAVED_MODEL_PATH)

model=tf.keras.models.load_model("model5.h5")

# PRECISION, RECALL, F1-SCORE

pred = model.predict(X_test)


def maxIndex(arr) :
    mx = 0
    mxIndx = -1
    for indx, val in enumerate(arr):
        if val > mx:
            mx = val
            mxIndx = indx
    return mxIndx
 
total_commands = 10
pos_reg = np.zeros(total_commands)
fal_reg = np.zeros(total_commands)
total = np.zeros(total_commands)
 
for indx, res in enumerate(pred):
    target = int(y_test[indx])
    
    total[target] += 1
    pred_class = maxIndex(res)
    
    if target == pred_class :
        pos_reg[target] += 1
    else:
        fal_reg[pred_class] += 1
 
# print(total, '\n', pos_reg,'\n',fal_reg)

s1=0
s2=0
s3=0
for i in range(total_commands):
    s1+=pos_reg[i]
    s2+=total[i]
    s3+=fal_reg[i]
 
# print(s1,"  ",s2,"  ",s3);
print("precesion: ",s1/(s1+s3)," recall: ",s1/s2, "f1 score :",(2*(s1/(s1+s3)))*(s1/s2)/((s1/(s1+s3))+(s1/s2)))
print()
 
for i in range(total_commands):
    print(i, ": ", (pos_reg[i] / total[i] * 100),"  precision : ",pos_reg[i]/(pos_reg[i]+fal_reg[i])," recall : ",pos_reg[i]/total[i]," f1score: ", (2*(pos_reg[i]/(pos_reg[i]+fal_reg[i]))*(pos_reg[i]/total[i]))/((pos_reg[i]/(pos_reg[i]+fal_reg[i]))+(pos_reg[i]/total[i])))

SAVED_MODEL_PATH = "model5.h5"
SAMPLE_RATE = 44080
TRACK_DURATION = 3 # measured in seconds
SAMPLES_PER_TRACK = SAMPLE_RATE * TRACK_DURATION

class _Keyword_Spotting_Service:
   

    model = None
    _mapping = [
        "এক নাম্বার বাতি নিভাও",
        "এক নাম্বার বাতি জ্বালানো হোক",
        "দুই নাম্বার বাতি নিভাও",
        "এক নাম্বার পাখা বন্ধ করো",
        "এক নাম্বার পাখা চালাও",
        "দরজা খোলো",
        "দুই নাম্বার পাখা চালাও",
        "দরজা বন্ধ করো",
        "দুই নাম্বার বাতি জ্বালাও",
        "দুই নাম্বার পাখা বন্ধ করো"
    ]
    _instance = None


    def predict(self, file_path):
        

        
        MFCCs = self.preprocess(file_path)

        
        MFCCs = MFCCs[np.newaxis, ..., np.newaxis]

        
        predictions = self.model.predict(MFCCs)
        predicted_index = np.argmax(predictions)
        predicted_keyword = self._mapping[predicted_index]
        return predicted_keyword


    def preprocess(self, file_path, num_mfcc=13, n_fft=2048, hop_length=512,num_segments=1):
       
        samples_per_segment = int(SAMPLES_PER_TRACK / num_segments)
        num_mfcc_vectors_per_segment = math.ceil(samples_per_segment / hop_length)
        signal, sample_rate = librosa.load(file_path, sr=SAMPLE_RATE)

        for d in range(num_segments):

                    
                    start = samples_per_segment * d
                    finish = start + samples_per_segment

                    signal = nr.reduce_noise(y=signal[start:finish], sr=sample_rate)

                    
                    MFCCs = librosa.feature.mfcc(signal, sample_rate, n_mfcc=num_mfcc, n_fft=n_fft, hop_length=hop_length)
                    mfcc = MFCCs.T

                    
        return mfcc


def Keyword_Spotting_Service():
    

    
    if _Keyword_Spotting_Service._instance is None:
        _Keyword_Spotting_Service._instance = _Keyword_Spotting_Service()
        _Keyword_Spotting_Service.model = tf.keras.models.load_model(SAVED_MODEL_PATH)
    return _Keyword_Spotting_Service._instance




if __name__ == "__main__":

    # create 2 instances of the keyword spotting service
    kss = Keyword_Spotting_Service()

    # make a prediction
    
    
    
    keyword17 = kss.predict("/content/drive/MyDrive/AudioDataset/Maindataset/Test/duibatijalao(105).wav")
    keyword18 = kss.predict("/content/drive/MyDrive/AudioDataset/Maindataset/Test/duibatinibao(12).wav")
    keyword19 = kss.predict("/content/drive/MyDrive/AudioDataset/Maindataset/Test/duipakhaoff(26).wav")
    keyword20 = kss.predict("/content/drive/MyDrive/AudioDataset/Maindataset/Test/ekbatinibao(4).wav")
    
    
    
    print(keyword17)
    print(keyword18)
    print(keyword19)
    print(keyword20)

from keras.models import load_model
import tensorflow as tf

model = load_model("model5.h5")


converter = tf.lite.TFLiteConverter.from_keras_model(model)
tflite_model = converter.convert()

print("model converted")

# Save the model.
with open('model5.tflite', 'wb') as f:
  f.write(tflite_model)

!apt-get install libsox-fmt-all libsox-dev sox > /dev/null
! pip install torchaudio > /dev/null
! python -m pip install git+https://github.com/facebookresearch/WavAugment.git > /dev/null

!pip install ffmpeg-python > /dev/null

import torchaudio
# Download example from WavAugment
! wget https://raw.githubusercontent.com/facebookresearch/WavAugment/master/tests/test.wav > /dev/null

# and load it as a tensor
x, sr = torchaudio.load('test.wav')

# code taken from https://ricardodeazambuja.com/deep_learning/2019/03/09/audio_and_video_google_colab/
from IPython.display import HTML, Audio
from google.colab.output import eval_js
from base64 import b64decode
import numpy as np
import io
import ffmpeg
import tempfile
import pathlib


AUDIO_HTML = """
<script>
var my_div = document.createElement("DIV");
var my_p = document.createElement("P");
var my_btn = document.createElement("BUTTON");
var t = document.createTextNode("Press to start recording");

my_btn.appendChild(t);
//my_p.appendChild(my_btn);
my_div.appendChild(my_btn);
document.body.appendChild(my_div);

var base64data = 0;
var reader;
var recorder, gumStream;
var recordButton = my_btn;

var handleSuccess = function(stream) {
  gumStream = stream;
  var options = {
    //bitsPerSecond: 8000, //chrome seems to ignore, always 48k
    mimeType : 'audio/webm;codecs=opus'
    //mimeType : 'audio/webm;codecs=pcm'
  };            
  //recorder = new MediaRecorder(stream, options);
  recorder = new MediaRecorder(stream);
  recorder.ondataavailable = function(e) {            
    var url = URL.createObjectURL(e.data);
    var preview = document.createElement('audio');
    preview.controls = true;
    preview.src = url;
    document.body.appendChild(preview);

    reader = new FileReader();
    reader.readAsDataURL(e.data); 
    reader.onloadend = function() {
      base64data = reader.result;
      //console.log("Inside FileReader:" + base64data);
    }
  };
  recorder.start();
  };

recordButton.innerText = "Recording... press to stop";

navigator.mediaDevices.getUserMedia({audio: true}).then(handleSuccess);


function toggleRecording() {
  if (recorder && recorder.state == "recording") {
      recorder.stop();
      gumStream.getAudioTracks()[0].stop();
      recordButton.innerText = "Saving the recording... pls wait!"
  }
}

// https://stackoverflow.com/a/951057
function sleep(ms) {
  return new Promise(resolve => setTimeout(resolve, ms));
}

var data = new Promise(resolve=>{
//recordButton.addEventListener("click", toggleRecording);
recordButton.onclick = ()=>{
toggleRecording()

sleep(2000).then(() => {
  // wait 2000ms for the data to be available...
  // ideally this should use something like await...
  //console.log("Inside data:" + base64data)
  resolve(base64data.toString())

});

}
});
      
</script>
"""

def get_audio():
  display(HTML(AUDIO_HTML))
  data = eval_js("data")
  binary = b64decode(data.split(',')[1])
  
  process = (ffmpeg
    .input('pipe:0')
    .output('pipe:1', format='wav')
    .run_async(pipe_stdin=True, pipe_stdout=True, pipe_stderr=True, quiet=True, overwrite_output=True)
  )
  output, err = process.communicate(input=binary)
  
  riff_chunk_size = len(output) - 8
  # Break up the chunk size into four bytes, held in b.
  q = riff_chunk_size
  b = []
  for i in range(4):
      q, r = divmod(q, 256)
      b.append(r)

  # Replace bytes 4:8 in proc.stdout with the actual size of the RIFF chunk.
  riff = output[:4] + bytes(b) + output[8:]

  with tempfile.TemporaryDirectory() as tmpdirname:
    path = pathlib.Path(tmpdirname) / 'tmp.wav'
    with open(path, 'wb') as f:
       f.write(riff)
       
    x, sr = torchaudio.load(path)

  return x, sr

x, sr = get_audio()

torchaudio.save("/content/drive/MyDrive/Recordedfiles/Inputfiles/myfile.wav", x, sr)
save_mfcc("/content/drive/MyDrive/Recordedfiles", "/content/drive/MyDrive/Recordedfilesjson/myrecorded.json", num_segments=1)
with open("/content/drive/MyDrive/Recordedfilesjson/myrecorded.json", "r") as fp:
    data = json.load(fp)

x = np.array(data["mfcc"])
y = np.array(data["labels"])

x = x[..., np.newaxis]
pred = model.predict(x)
print(pred)
mxval = maxIndex(pred[0])

print("Predicted Class:: ", mxval)